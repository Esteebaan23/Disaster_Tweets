{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqmD_r09NE0M",
        "outputId": "c6ef3f3c-6fbc-4f51-b082-767ced5abad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      id      keyword                     location  \\\n",
            "0   3796  destruction                          NaN   \n",
            "1   3185       deluge                          NaN   \n",
            "2   7769       police                           UK   \n",
            "3    191   aftershock                          NaN   \n",
            "4   9810       trauma        Montgomery County, MD   \n",
            "..   ...          ...                          ...   \n",
            "95  2038   casualties  50% Queanbeyan - 50% Sydney   \n",
            "96  1526  body%20bags   Speaking the Truth in Love   \n",
            "97   463   armageddon                          NaN   \n",
            "98  4121      drought                Charlotte, NC   \n",
            "99  9690      tornado                          NaN   \n",
            "\n",
            "                                                 text  target  \n",
            "0   So you have a new weapon that can cause un-ima...       1  \n",
            "1   The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
            "2   DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
            "3   Aftershock back to school kick off was great. ...       0  \n",
            "4   in response to trauma Children of Addicts deve...       0  \n",
            "..                                                ...     ...  \n",
            "95  @FlyOpineMonkey You mean Olympic ;-) \\nAlso it...       1  \n",
            "96  Fairfax investigating firefighter over Faceboo...       0  \n",
            "97  RT @RTRRTcoach: #Love #TrueLove #romance lith ...       0  \n",
            "98  BLOG: Rain much needed as drought conditions w...       1  \n",
            "99  @Rebelmage2 I'm glad you got away XD But My 'b...       1  \n",
            "\n",
            "[100 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "test  = pd.read_csv('/content/test.csv')\n",
        "\n",
        "\n",
        "# Reduce both datasets to 1000 entries each\n",
        "train = train.sample(n=100, random_state=42).reset_index(drop=True)\n",
        "test = test.sample(n=100, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCyGvlckMXqy"
      },
      "outputs": [],
      "source": [
        "# # Replace missing values in 'keyword' and 'location' with empty strings\n",
        "# train['keyword'].fillna('', inplace=True)\n",
        "# train['location'].fillna('', inplace=True)\n",
        "\n",
        "# # Calculate the percentage of missing values after replacement\n",
        "# missing_keyword = train['keyword'].isnull().mean() * 100\n",
        "# missing_location = train['location'].isnull().mean() * 100\n",
        "\n",
        "# print(f\"Percentage of missing 'keyword': {missing_keyword}%\")\n",
        "# print(f\"Percentage of missing 'location': {missing_location}%\")\n",
        "\n",
        "# # Check for any missing values in the 'keyword' and 'location' columns after replacing\n",
        "# print(\"Missing values in 'keyword':\", train['keyword'].isnull().sum())\n",
        "# print(\"Missing values in 'location':\", train['location'].isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aRaEi74QH7J",
        "outputId": "83c07771-7c4e-4e11-a548-07116381726b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of missing 'keyword': 1.0%\n",
            "Percentage of missing 'location': 40.0%\n",
            "Missing values in 'keyword': 1\n",
            "Missing values in 'location': 40\n"
          ]
        }
      ],
      "source": [
        "# Calculate percentage of missing values\n",
        "missing_keyword = train['keyword'].isnull().mean() * 100\n",
        "missing_location = train['location'].isnull().mean() * 100\n",
        "print(f\"Percentage of missing 'keyword': {missing_keyword}%\")\n",
        "print(f\"Percentage of missing 'location': {missing_location}%\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Missing values in 'keyword':\", train['keyword'].isnull().sum())\n",
        "print(\"Missing values in 'location':\", train['location'].isnull().sum())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKTQ_99xeAKB",
        "outputId": "d25982af-6378-4603-fc2a-003b6bed13b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if you haven't\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove stopwords\n",
        "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
        "\n",
        "    # Remove emojis (using a Unicode range for emojis)\n",
        "    text = re.sub(r'[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]', '', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA4XpvZ6eDQy",
        "outputId": "4843b55d-db58-4af6-8d17-6e68ad618830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  \\\n",
            "0  So you have a new weapon that can cause un-ima...   \n",
            "1  The f$&amp;@ing things I do for #GISHWHES Just...   \n",
            "2  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...   \n",
            "3  Aftershock back to school kick off was great. ...   \n",
            "4  in response to trauma Children of Addicts deve...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0          new weapon cause unimaginable destruction  \n",
            "1  famping things GISHWHES got soaked deluge goin...  \n",
            "2  DT georgegalloway RT Galloway4Mayor ÛÏThe CoL...  \n",
            "3  Aftershock back school kick great want thank e...  \n",
            "4  response trauma Children Addicts develop defen...  \n"
          ]
        }
      ],
      "source": [
        "train['cleaned_text'] = train['text'].apply(clean_text)\n",
        "\n",
        "# Display the cleaned text\n",
        "print(train[['text', 'cleaned_text']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vGt7894NNS6",
        "outputId": "6fe5fc03-eedb-49d7-a343-b729f0abed44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  \\\n",
            "0  So you have a new weapon that can cause un-ima...   \n",
            "1  The f$&amp;@ing things I do for #GISHWHES Just...   \n",
            "2  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...   \n",
            "3  Aftershock back to school kick off was great. ...   \n",
            "4  in response to trauma Children of Addicts deve...   \n",
            "\n",
            "                                         bert_tokens  \\\n",
            "0  [so, you, have, a, new, weapon, that, can, cau...   \n",
            "1  [the, f, $, &, amp, ;, @, ing, things, i, do, ...   \n",
            "2  [dt, @, george, ##gall, ##owa, ##y, :, rt, @, ...   \n",
            "3  [after, ##sho, ##ck, back, to, school, kick, o...   \n",
            "4  [in, response, to, trauma, children, of, addic...   \n",
            "\n",
            "                                      bert_token_ids  \n",
            "0  [2061, 2017, 2031, 1037, 2047, 5195, 2008, 206...  \n",
            "1  [1996, 1042, 1002, 1004, 23713, 1025, 1030, 13...  \n",
            "2  [26718, 1030, 2577, 22263, 21293, 2100, 1024, ...  \n",
            "3  [2044, 22231, 3600, 2067, 2000, 2082, 5926, 21...  \n",
            "4  [1999, 3433, 2000, 12603, 2336, 1997, 26855, 2...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "# Load the pretrained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a function to tokenize the text and convert to IDs\n",
        "def bert_tokenize(text):\n",
        "    # Tokenize the text using BERT's tokenizer\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Convert tokens to IDs\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    return tokens, token_ids\n",
        "\n",
        "# Apply the function to your dataset\n",
        "train['bert_tokens'], train['bert_token_ids'] = zip(*train['text'].apply(bert_tokenize))\n",
        "\n",
        "# Display the first few rows to check the output\n",
        "print(train[['text', 'bert_tokens', 'bert_token_ids']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DPm-cjaR-QA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Define max sequence length\n",
        "MAX_LEN = 64\n",
        "\n",
        "# Pad the token IDs and create attention masks\n",
        "input_ids = list(train['bert_token_ids'].apply(lambda x: x + [0] * (MAX_LEN - len(x)) if len(x) < MAX_LEN else x[:MAX_LEN]))\n",
        "attention_masks = [[float(i > 0) for i in seq] for seq in input_ids]\n",
        "\n",
        "# Convert lists to tensors\n",
        "input_ids = torch.tensor(input_ids)\n",
        "attention_masks = torch.tensor(attention_masks)\n",
        "labels = torch.tensor(train['target'].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5nvj8TxSCbH"
      },
      "outputs": [],
      "source": [
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.1, random_state=42)\n",
        "train_masks, val_masks = train_test_split(attention_masks, test_size=0.1, random_state=42)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Create DataLoader for training and validation\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6XPEDCqSJ1n",
        "outputId": "f8eedc38-d228-477f-eb70-31dc12fc8728"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc6iDQ2bSOHF",
        "outputId": "11f8dbec-f8f8-4402-89c0-7c216efee892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "Average training loss: 0.6641874810059866\n",
            "Validation accuracy: 0.375\n",
            "Validation loss: 0.7677536010742188\n",
            "Epoch 2/2\n",
            "Average training loss: 0.6283511320749918\n",
            "Validation accuracy: 0.375\n",
            "Validation loss: 0.7880755066871643\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Set number of epochs\n",
        "epochs = 2\n",
        "\n",
        "# Total steps for the scheduler\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Scheduler for learning rate decay\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Clear gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Average training loss: {avg_train_loss}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Turn off gradient calculations for validation\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean()\n",
        "        total_eval_accuracy += accuracy\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
        "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "\n",
        "    print(f\"Validation accuracy: {avg_val_accuracy}\")\n",
        "    print(f\"Validation loss: {avg_val_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SNwm5qiGpzB",
        "outputId": "93549e21-cc26-4960-f5bb-32de17d976af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/tweetModel.zip'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Save model and tokenizer in Colab\n",
        "save_directory = \"/content/tweetModel\"\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "# Zip the folder containing the model and tokenizer\n",
        "shutil.make_archive(save_directory, 'zip', save_directory)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QWrAm2goLU2K"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess and predict tweet sentiment\n",
        "def predict_tweet_sentiment(tweet):\n",
        "    # Tokenize the input tweet\n",
        "    inputs = tokenizer(tweet, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
        "\n",
        "    # Move inputs to GPU if available (optional)\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {key: value.cuda() for key, value in inputs.items()}\n",
        "\n",
        "    # Model prediction\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Get the predicted class (0 or 1)\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Return the prediction\n",
        "    return \"Disaster\" if prediction == 1 else \"Not Disaster\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGlDejitLXCi"
      },
      "outputs": [],
      "source": [
        "# Test the function by entering a tweet\n",
        "tweet = input(\"Enter a tweet to classify: \")\n",
        "prediction = predict_tweet_sentiment(tweet)\n",
        "print(f\"Prediction: {prediction}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZXErn8DEG_w"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Validation phase\n",
        "model.eval()\n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for batch in val_dataloader:\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Turn off gradient calculations for validation\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits\n",
        "\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    preds = torch.argmax(logits, dim=1).flatten()\n",
        "    accuracy = (preds == b_labels).cpu().numpy().mean()\n",
        "    total_eval_accuracy += accuracy\n",
        "\n",
        "    # Collect all predictions and labels for F1 score calculation\n",
        "    all_preds.extend(preds.cpu().numpy())\n",
        "    all_labels.extend(b_labels.cpu().numpy())\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')  # or use 'macro' for macro-average\n",
        "avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
        "avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "\n",
        "print(f\"Validation accuracy: {avg_val_accuracy}\")\n",
        "print(f\"Validation loss: {avg_val_loss}\")\n",
        "print(f\"Validation F1 score: {f1}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}