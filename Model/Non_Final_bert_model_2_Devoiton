import pandas as pd
import numpy as np
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.metrics import f1_score, confusion_matrix
from datasets import Dataset
import nltk

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Define text cleaning function
def clean_text(text):
    # Remove special characters and digits
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Convert to lowercase
    text = text.lower()

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    # Tokenize and remove stopwords
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Join tokens back into a string
    cleaned_text = ' '.join(tokens)

    return cleaned_text

# Load the train CSV file
train_file_path = '/content/train.csv'  # Update the path as needed
df_train = pd.read_csv(train_file_path, encoding='ISO-8859-1')

# Drop unnecessary columns
columns_to_drop = ['keyword', 'location', 'id']  # Columns to drop
df_train.drop(columns=columns_to_drop, errors='ignore', inplace=True)

# Check and clean the dataset
if 'target' in df_train.columns:
    df_train = df_train.rename(columns={'target': 'labels'})  # Rename target column to labels
df_train = df_train[['text', 'labels']]  # Retain only relevant columns
df_train.dropna(subset=['text', 'labels'], inplace=True)  # Remove rows with missing values
df_train['labels'] = df_train['labels'].astype(int)  # Ensure labels are integers
df_train['cleaned_text'] = df_train['text'].apply(clean_text)  # Clean text column

# Shuffle the dataset
df_train = df_train.sample(frac=1, random_state=42)

# Split data into training and testing sets
train_size = 0.8
train_df = df_train.sample(frac=train_size, random_state=42)
test_df = df_train.drop(train_df.index)

print(f"\nTraining samples: {len(train_df)}")
print(f"Testing samples: {len(test_df)}")

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_pandas(train_df[['cleaned_text', 'labels']])
test_dataset = Dataset.from_pandas(test_df[['cleaned_text', 'labels']])

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')  # Use DistilBERT for faster training
model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

# Tokenize datasets
def tokenize_function(examples):
    return tokenizer(examples['cleaned_text'], padding="max_length", truncation=True, max_length=64)

tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=["cleaned_text"])
tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=["cleaned_text"])

# Define metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "f1": f1_score(labels, predictions, average="weighted"),
        "confusion_matrix": confusion_matrix(labels, predictions).tolist(),
    }

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    fp16=True,
    save_total_limit=1,
    report_to="none",
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    compute_metrics=compute_metrics,
)

# Train the model
print("\nStarting training...")
trainer.train()
print("\nTraining complete.")

# Evaluate the model
print("\nEvaluating the model...")
eval_results = trainer.evaluate()
print("\nEvaluation Results:")
print(eval_results)

# Generate predictions and confusion matrix
predictions = trainer.predict(tokenized_test)
logits = predictions.predictions
true_labels = predictions.label_ids
predicted_labels = np.argmax(logits, axis=-1)

conf_matrix = confusion_matrix(true_labels, predicted_labels)

# Visualize the confusion matrix
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Disaster', 'Disaster'], yticklabels=['Non-Disaster', 'Disaster'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Save the fine-tuned model and tokenizer
model.save_pretrained('./fine_tuned_distilbert_model')
tokenizer.save_pretrained('./fine_tuned_distilbert_model')

print("\nFine-tuned model and tokenizer saved successfully.")
